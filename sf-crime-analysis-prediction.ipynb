{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":false,"_uuid":"abd80502f1cfc3bed6402f86ef9374f97c845c7a","trusted":true},"outputs":[],"source":["import pandas as pd\n","from shapely.geometry import  Point\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","from matplotlib import cm\n","import urllib.request\n","import shutil\n","import zipfile\n","import os\n","import re\n","# import contextily as ctx\n","# import geoplot as gplt\n","import lightgbm as lgb\n","import eli5\n","from eli5.sklearn import PermutationImportance\n","from lightgbm import LGBMClassifier\n","from matplotlib import pyplot as plt\n","# from pdpbox import pdp, get_dataset, info_plots\n","# import shap"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"_kg_hide-input":true,"_uuid":"f2d723c29db2f8e8565887f2a784704b1f60b572","trusted":true},"outputs":[],"source":["train = pd.read_csv('./crime-data/train.csv.zip', parse_dates=['Dates'])\n","test = pd.read_csv('./crime-data/test.csv.zip', parse_dates=['Dates'], index_col='Id')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"66c966319fa299f29c9260cf880d8ef97296e99f","trusted":true},"outputs":[],"source":["print('First date: ', str(train.Dates.describe()['first']))\n","print('Last date: ', str(train.Dates.describe()['last']))\n","print('Test data shape ', train.shape)"]},{"cell_type":"markdown","metadata":{"_uuid":"1f4ed3b23ae84cc7a8a1def77eb7224ebcff8a21"},"source":["The data ranges from 1/1/2003 to 5/13/2015 creating a training dataset with nine features and 878,049 samples"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"b89bfc9c8de63768af8cefc488992ef6597e2c7e","trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"650616254a8a01d196a5fe8196c9cf71ab634f60"},"source":["More specifically it includes the following variables.\n","* Dates - timestamp of the crime incident\n","* Category - category of the crime incident. (This is our target variable.)\n","* Descript - detailed description of the crime incident\n","* DayOfWeek - the day of the week\n","* PdDistrict - the name of the Police Department District\n","* Resolution - The resolution of the crime incident\n","* Address - the approximate street address of the crime incident \n","* X - Longitude\n","* Y - Latitude\n"]},{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"990b17a49cbb4990e31689aaf97545504f91a9b9","trusted":true},"outputs":[{"data":{"text/plain":["array(['WARRANTS', 'OTHER OFFENSES', 'LARCENY/THEFT', 'VEHICLE THEFT',\n","       'VANDALISM', 'NON-CRIMINAL', 'ROBBERY', 'ASSAULT', 'WEAPON LAWS',\n","       'BURGLARY', 'SUSPICIOUS OCC', 'DRUNKENNESS',\n","       'FORGERY/COUNTERFEITING', 'DRUG/NARCOTIC', 'STOLEN PROPERTY',\n","       'SECONDARY CODES', 'TRESPASS', 'MISSING PERSON', 'FRAUD',\n","       'KIDNAPPING', 'RUNAWAY', 'DRIVING UNDER THE INFLUENCE',\n","       'SEX OFFENSES FORCIBLE', 'PROSTITUTION', 'DISORDERLY CONDUCT',\n","       'ARSON', 'FAMILY OFFENSES', 'LIQUOR LAWS', 'BRIBERY',\n","       'EMBEZZLEMENT', 'SUICIDE', 'LOITERING',\n","       'SEX OFFENSES NON FORCIBLE', 'EXTORTION', 'GAMBLING', 'BAD CHECKS',\n","       'TREA', 'RECOVERED VEHICLE', 'PORNOGRAPHY/OBSCENE MAT'],\n","      dtype=object)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train['Category'].unique()"]},{"cell_type":"markdown","metadata":{"_uuid":"1b5d238165631f96b0dd7caea1104c8f21a76bf4"},"source":["The dataset contains a lot of 'object' variables (aka strings) that we will need to encode."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c1ee420a26c17a509d20ed82bc65d634d047b8aa","trusted":true},"outputs":[],"source":["train.duplicated().sum()"]},{"cell_type":"markdown","metadata":{"_uuid":"a68a9e7bc55893d29574fca90e4121c35266aad6"},"source":["It also contains 2323 duplicates that we should remove.  \n","We will also evaluate the position of the data points using the coordinates."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a965b1444083b292a72f517cc2f98158f5f56234","trusted":true},"outputs":[],"source":["def create_gdf(df):\n","    gdf = df.copy()\n","    gdf['Coordinates'] = list(zip(gdf.X, gdf.Y))\n","    gdf.Coordinates = gdf.Coordinates.apply(Point)\n","    gdf = gpd.GeoDataFrame(\n","        gdf, geometry='Coordinates', crs={'init': 'epsg:4326'})\n","    return gdf\n","\n","train_gdf = create_gdf(train)\n","\n","world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n","ax = world.plot(color='white', edgecolor='black')\n","train_gdf.plot(ax=ax, color='red')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"fed40706190c708d2200157aff985a802633c20a"},"source":["Some points are misplaced. Let's see how many they are."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"90e3a7c2bd2be831452cde95b145d00e920021be","trusted":true},"outputs":[],"source":["print(train_gdf.loc[train_gdf.Y > 50].count()[0])\n","train_gdf.loc[train_gdf.Y > 50].sample(5)"]},{"cell_type":"markdown","metadata":{"_uuid":"87e4b75f0275365ae8992f9d1c166d010cd39198"},"source":["We will replace the outlying coordinates with the average coordinates of the district they belong."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2bc2ff9ba64243f70c8ba72a13e2744a7658ccdd","trusted":true},"outputs":[],"source":["train.drop_duplicates(inplace=True)\n","train.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n","test.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n","\n","imp = SimpleImputer(strategy='mean')\n","\n","for district in train['PdDistrict'].unique():\n","    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n","        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n","    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n","        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\n","\n","train_gdf = create_gdf(train)"]},{"cell_type":"markdown","metadata":{"_uuid":"e6fa13315fbd22a8417de5e99e265465b4faa90d"},"source":["After cleaning the dataset from outliers and duplicates, we examine the variables.  \n","### Dates & Day of the week  \n","These variables are distributed uniformly between 1/1/2003 to 5/13/2015 (and Monday to Sunday) and split between the training and the testing dataset as mentioned before. We did not notice any anomalies on these variables.  \n","The median frequency of incidents is 389 per day with a standard deviation of 48.51."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1c9ec328a2beeb811020bd018356d832a8dd79ed","trusted":true},"outputs":[],"source":["col = sns.color_palette()\n","\n","train['Date'] = train.Dates.dt.date\n","train['Hour'] = train.Dates.dt.hour\n","\n","plt.figure(figsize=(10, 6))\n","data = train.groupby('Date').count().iloc[:, 0]\n","sns.kdeplot(data=data, shade=True)\n","plt.axvline(x=data.median(), ymax=0.95, linestyle='--', color=col[1])\n","plt.annotate(\n","    'Median: ' + str(data.median()),\n","    xy=(data.median(), 0.004),\n","    xytext=(200, 0.005),\n","    arrowprops=dict(arrowstyle='->', color=col[1], shrinkB=10))\n","plt.title(\n","    'Distribution of number of incidents per day', fontdict={'fontsize': 16})\n","plt.xlabel('Incidents')\n","plt.ylabel('Density')\n","plt.legend().remove()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"815d0407f51fcd919cf9a6f03a8d0c0bda4062bc"},"source":["Also, there is no significant deviation of incidents frequency throughout the week. Thus we do not expect this variable to play a significant role in the prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4e5c858139ed1bef557d9895852dc544698a4cd7","trusted":true},"outputs":[],"source":["data = train.groupby('DayOfWeek').count().iloc[:, 0]\n","data = data.reindex([\n","    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n","    'Sunday'\n","])\n","\n","plt.figure(figsize=(10, 5))\n","with sns.axes_style(\"whitegrid\"):\n","    ax = sns.barplot(\n","        data.index, (data.values / data.values.sum()) * 100,\n","        orient='v',\n","        palette=cm.ScalarMappable(cmap='Reds').to_rgba(data.values))\n","\n","plt.title('Incidents per Weekday', fontdict={'fontsize': 16})\n","plt.xlabel('Weekday')\n","plt.ylabel('Incidents (%)')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"46436f6ab08d64bd6bb4b463a615b83287a29a85"},"source":["### Category  \n","There are 39 discrete categories that the police department file the incidents with the most common being Larceny/Theft (19.91%), Non/Criminal (10.50%), and Assault(8.77%)."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"316ad684669edfdc3064728f26c2122a09b97083","trusted":true},"outputs":[],"source":["data = train.groupby('Category').count().iloc[:, 0].sort_values(\n","    ascending=False)\n","data = data.reindex(np.append(np.delete(data.index, 1), 'OTHER OFFENSES'))\n","\n","plt.figure(figsize=(10, 10))\n","with sns.axes_style(\"whitegrid\"):\n","    ax = sns.barplot(\n","        (data.values / data.values.sum()) * 100,\n","        data.index,\n","        orient='h',\n","        palette=\"Reds_r\")\n","\n","plt.title('Incidents per Crime Category', fontdict={'fontsize': 16})\n","plt.xlabel('Incidents (%)')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"48262500e83132e28d2ef5853a29f622b312ab76"},"source":["### Police District  \n","There are significant differences between the different districts of the City with the Southern district having the most incidents (17.87%) followed by Mission (13.67%) and Northern (12.00%)."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5fc41a1caeb561517721483016f5ac7aaf4e0e13","trusted":true},"outputs":[],"source":["# Downloading the shapefile of the area \n","url = 'https://data.sfgov.org/api/geospatial/wkhw-cjsf?method=export&format=Shapefile'\n","with urllib.request.urlopen(url) as response, open('pd_data.zip', 'wb') as out_file:\n","    shutil.copyfileobj(response, out_file)\n","# Unzipping it\n","with zipfile.ZipFile('pd_data.zip', 'r') as zip_ref:\n","    zip_ref.extractall('pd_data')\n","# Loading to a geopandas dataframe\n","for filename in os.listdir('./pd_data/'):\n","    if re.match(\".+\\.shp\", filename):\n","        pd_districts = gpd.read_file('./pd_data/'+filename)\n","        break\n","# Defining the coordinate system to longitude/latitude\n","pd_districts.crs={'init': 'epsg:4326'}\n","\n","# Merging our train dataset with the geo-dataframe\n","pd_districts = pd_districts.merge(\n","    train.groupby('PdDistrict').count().iloc[:, [0]].rename(\n","        columns={'Dates': 'Incidents'}),\n","    how='inner',\n","    left_on='district',\n","    right_index=True,\n","    suffixes=('_x', '_y'))\n","\n","# Transforming the coordinate system to Spherical Mercator for\n","# compatibility with the tiling background\n","pd_districts = pd_districts.to_crs({'init': 'epsg:3857'})\n","\n","# Calculating the incidents per day for every district\n","train_days = train.groupby('Date').count().shape[0]\n","pd_districts['inc_per_day'] = pd_districts.Incidents/train_days\n","\n","# Ploting the data\n","fig, ax = plt.subplots(figsize=(10, 10))\n","pd_districts.plot(\n","    column='inc_per_day',\n","    cmap='Reds',\n","    alpha=0.6,\n","    edgecolor='r',\n","    linestyle='-',\n","    linewidth=1,\n","    legend=True,\n","    ax=ax)\n","\n","def add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n","    \"\"\"Function that add the tile background to the map\"\"\"\n","    xmin, xmax, ymin, ymax = ax.axis()\n","    basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n","    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n","    # restore original x/y limits\n","    ax.axis((xmin, xmax, ymin, ymax))\n","\n","# Adding the background\n","add_basemap(ax, zoom=11, url=ctx.sources.ST_TONER_LITE)\n","\n","# Adding the name of the districts\n","for index in pd_districts.index:\n","    plt.annotate(\n","        pd_districts.loc[index].district,\n","        (pd_districts.loc[index].geometry.centroid.x,\n","         pd_districts.loc[index].geometry.centroid.y),\n","        color='#353535',\n","        fontsize='large',\n","        fontweight='heavy',\n","        horizontalalignment='center'\n","    )\n","\n","ax.set_axis_off()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"7326225c0886d42f59e09ee921374e4ce33c4ccf"},"source":["### Address  \n","Address, as a text field, requires advanced techniques to use it for the prediction. Instead in this project, we will use it to extract if the incident has happened on the road or in a building block.  \n","### X - Longitude Y - Latitude  \n","We have tested that the coordinates belong inside the boundaries of the city. Although longitude does not contain any outliers, latitude includes some 90o values which correspond to the North Pole. \n","## Exploratory Visualization\n","Based on the Project’s statement, we need to predict the probability of each type of crime based on time and location. That being said, we present two diagrams to visualize the importance of these variables.\n","The first one presents the geographic density of 9 random crime categories. We can see that although the epicenter of most of the crimes resides on the northeast of the city, each crime has a different density on the rest of the city. This fact is a reliable indication that the location ( coordinates / Police District) will be a significant factor for the analysis and the forecasting. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["crimes = train['Category'].unique().tolist()\n","crimes.remove('TREA')\n","\n","pd_districts = pd_districts.to_crs({'init':'epsg:4326'})\n","sf_land = pd_districts.unary_union\n","sf_land = gpd.GeoDataFrame(gpd.GeoSeries(sf_land), crs={'init':'epsg:4326'})\n","sf_land = sf_land.rename(columns={0:'geometry'}).set_geometry('geometry')\n","\n","fig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(12,12))\n","for i , crime in enumerate(np.random.choice(crimes, size=9, replace=False)):\n","    data = train_gdf.loc[train_gdf['Category'] == crime]\n","    ax = fig.add_subplot(3, 3, i+1)\n","    gplt.kdeplot(data,\n","                 shade=True,\n","                 shade_lowest=False,\n","                 clip = sf_land.geometry,\n","                 cmap='Reds',\n","                 ax=ax)\n","    gplt.polyplot(sf_land, ax=ax)\n","    ax.set_title(crime) \n","plt.suptitle('Geographic Density of Different Crimes')\n","fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"f45fedf76e84b277be00f615b69d84f27c11d384"},"source":["The second diagram presents the average number of incidents per hour for five of the crimes' categories. It is evident that different crimes have different frequency during different times of the day. \n","Some examples are that prostitution picks during the evening and all through the night, Gambling incidents start late at night until the morning and Burglary picks early in the morning until the afternoon.\n","As before these are sharp pieces of evidence that the time parameters will have a significant role also.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"41cfac02b5b3b373d14c47f3062a0c8d1afdf65f","trusted":true},"outputs":[],"source":["data = train.groupby(['Hour', 'Date', 'Category'],\n","                     as_index=False).count().iloc[:, :4]\n","data.rename(columns={'Dates': 'Incidents'}, inplace=True)\n","data = data.groupby(['Hour', 'Category'], as_index=False).mean()\n","data = data.loc[data['Category'].isin(\n","    ['ROBBERY', 'GAMBLING', 'BURGLARY', 'ARSON', 'PROSTITUTION'])]\n","\n","sns.set_style(\"whitegrid\")\n","fig, ax = plt.subplots(figsize=(14, 4))\n","ax = sns.lineplot(x='Hour', y='Incidents', data=data, hue='Category')\n","ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\n","plt.suptitle('Average number of incidents per hour')\n","fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"f5fcc3692620e81993999f8de5e074bffb696caa"},"source":["## Algorithms and Techniques  \n","The specific problem is a typical multiclass classification problem, and there are several categories of algorithms for solving it. Initially, we evaluated several appropriate algorithms from Linear Models (Stochastic Gradient Descent), Nearest Neighbors (K nearest neighbors), Ensemble methods (Random Forests & AdaBoost) and Boosting Algorithms (XGBoost & LIghtGBM) using basic feature engineering and the default parameters to evaluate if any of them has a significant head start:  \n","\n","| **Algorithm**                   | **Parameters**                                        | **Logloss**  |\n","|:---------------------------:|:-------------------------------------------------:|:--------:|\n","| Stochastic Gradient Descent | Default Scikit-Learn Parameters (with 'log' loss) |  2.86631 |\n","| K-Nearest Neighbors         | Default Scikit-Learn Parameters                   | 23.29263 |\n","| Random Forest               | Default Scikit-Learn Parameters                   |  2.92716 |\n","| AdaBoost                    | Default Scikit-Learn Parameters                   |  3.58856 |\n","| XGBoost                     | Default Scikit-Learn Parameters                   |  2.91656 |\n","| LIghtGBM                    | Default Scikit-Learn Parameters                   |  2.98336 |  "]},{"cell_type":"markdown","metadata":{"_uuid":"1ca652ca38fde26b54965b4afb1e00ac7374e9de"},"source":["SGD scored the best initial result, but after a lengthy hyperparameter tuning, it was not able to pass a 2.54503 threshold.  \n","\n","Finally, from the algorithms that scored under 3.0, we decided to work with LightGBM due to its efficiency and versatility in the hyperparameters tuning.\n","LightGBM is a decision tree boosting algorithm uses histogram-based algorithms which bucket continuous feature (attribute) values into discrete bins. This technique speeds up training and reduces memory usage. In layman terms the algorithm works like this:  \n","\n","1. Fit a decision tree to the data\n","2. Evaluate the model\n","3. Increase the weight to the incorrect samples.\n","4. Choose the leaf with max delta loss to grow.\n","5. Grow the tree.\n","6. Go to step 2 ![lightgbm](https://lightgbm.readthedocs.io/en/latest/_images/leaf-wise.png) \n","\n","## Benchmark  \n","There are two types of benchmarks we need to set. The first will be a naive prediction. This prediction will be a baseline score to compare with our model’s score to evaluate if we have any significant progress.  \n","\n","In a Multiclass Classification, the best way to calculate the baseline is by assuming that the probability of each category equals its average frequency in the train set. The frequency can be calculated easily by dividing the sum of incidents of each category by the number of rows of the training set.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"793c149fc7a20583d1962cd4ee5eb5df433298cb","trusted":true},"outputs":[],"source":["naive_vals = train.groupby('Category').count().iloc[:,0]/train.shape[0]\n","n_rows = test.shape[0]\n","\n","submission = pd.DataFrame(\n","    np.repeat(np.array(naive_vals), n_rows).reshape(39, n_rows).transpose(),\n","    columns=naive_vals.index)"]},{"cell_type":"markdown","metadata":{"_uuid":"bb19d047d5bea62fb7056c83ffdec686aa823b60"},"source":["The baseline calculated this way is 2.68015. (Details in [SF-Crime Analysis & Prediction (Naive Prediction)](https://www.kaggle.com/yannisp/sf-crime-analysis-prediction-naive-prediction) Notebook. We can notice that this baseline is already lower than the initial score of our classifiers.  \n","\n","Another critical benchmark is usually the ‘Human Performance’, as a proxy for the Bayes error rate. The specific problem does not belong to a field that humans excel (like computer vision or NLP), so as a proxy for the  [Bayes error rate](https://en.wikipedia.org/wiki/Bayes_error_rate), we will use the score of the best kernel so far which is [initial benchmark need tuning](https://www.kaggle.com/sergeylebedev/initial-benchmark-need-tuning) by the user [Sergey Lebedev](https://www.kaggle.com/sergeylebedev) with score 2.29318. \n","\n","The small distance between the baseline score and the Bayes error rate indicate that this is a hard problem with a low margin of improvement.  \n","# Methodology\n","## Data Preprocessing\n","### Data Wrangling\n","Following the methodology described in the Problem Statement, we identified 2323 duplicate values and 67 wrong latitudes. The duplicates removed and the outliers imputed.  \n","### Feature Engineering\n","Then, we created additional features. More specifically:\n","* From the ‘Dates’ field, we extracted the Day, the Month, the Year, the Hour, the Minute, the Weekday, and the number of days since the first day in the data.\n","* From the ‘Address’ field we extracted if the incident has taken place in a crossroad or on a building block.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e9642aac981d00da7e6c7f98580e598400e97598","trusted":true},"outputs":[],"source":["def feature_engineering(data):\n","    data['Date'] = pd.to_datetime(data['Dates'].dt.date)\n","    data['n_days'] = (\n","        data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n","    data['Day'] = data['Dates'].dt.day\n","    data['DayOfWeek'] = data['Dates'].dt.weekday\n","    data['Month'] = data['Dates'].dt.month\n","    data['Year'] = data['Dates'].dt.year\n","    data['Hour'] = data['Dates'].dt.hour\n","    data['Minute'] = data['Dates'].dt.minute\n","    data['Block'] = data['Address'].str.contains('block', case=False)\n","    \n","    data.drop(columns=['Dates','Date','Address'], inplace=True)\n","        \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"97d7a3a2016f80d4635ce928194e391076e43307","trusted":true},"outputs":[],"source":["train = feature_engineering(train)\n","train.drop(columns=['Descript','Resolution'], inplace=True)\n","test = feature_engineering(test)\n","train.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"4b9968f90b11c1dac5864fdd521b7135b5a7ea6f"},"source":["### Feature Scaling\n","Deciding to continue with a tree-based algorithm there was no need for scaling on the final dataset.\n","### Feature Selection\n","After the feature engineering described above, we ended up with 11 features. To identify if any of them increased the complexity of the model without adding significant gain to the model, we used the method of Permutation Importance.  \n","\n","The idea is that the importance of a feature can be measured by looking at how much the loss decreases when a feature is not available. To do that we can remove each feature from the dataset, re-train the estimator and check the impact. Doing this would require re-training an estimator for each feature, which can be computationally intensive. Instead, we can replace it with noise by shuffle values for a feature.  \n","\n","The implementation of the above technique showed that there is no need for any feature removal since all of them have a positive impact in the dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5458f8193aeb168bddce69be0b1dca3d57326f5d","trusted":true},"outputs":[],"source":["le1 = LabelEncoder()\n","train['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n","test['PdDistrict'] = le1.transform(test['PdDistrict'])\n","le2 = LabelEncoder()\n","y = le2.fit_transform(train.pop('Category'))\n","\n","train_X, val_X, train_y, val_y = train_test_split(train, y)\n","\n","model =LGBMClassifier(objective='multiclass', num_class=39).fit(train_X, train_y)\n","\n","perm = PermutationImportance(model).fit(val_X, val_y)\n","eli5.show_weights(perm, feature_names=val_X.columns.tolist())"]},{"cell_type":"markdown","metadata":{"_uuid":"88d5f60166b3de09c089697e3fd02306a0990749"},"source":["## Building the Initial Model\n","To build the model we used the LightGBM’s Python API.\n","First we created the dataset by combining the features, the target and declaring the PdDistrict as a categorical variable using ‘lightgbm.Dataset()`.  \n","\n","Then we used Cross-Validation with early stopping (10 rounds) and parameters:  \n","* Objective = ‘'multiclass',  \n","* 'Metric = ‘multi_logloss',  \n","* 'Num_class = 39  \n","\n","The above setup achieved 2.46799  cross-validation score after 23 epochs and 2.49136 on the testing set.  \n","[SF-Crime Analysis & Prediction (Base Model)](https://www.kaggle.com/yannisp/sf-crime-analysis-prediction-base-model/notebook?scriptVersionId=9334889) \n","![base_model](https://i.imgur.com/AcJHphZ.png)\n","## Refinement  \n","Instead of the most popular methods of Exhaustive Grid Search and Randomized Parameter Optimization, we selected another more efficient way to tune the hyperparameters of the algorithm; **Bayesian optimization**.  \n","\n","The problem with the two techniques mentioned above is that they do not use previous results to pick the next input values. Instead, Bayesian optimization, also called Sequential Model-Based Optimization (SMBO), implements this idea by building a probability model of the objective function that maps input values to a probability of a loss: p (loss | input values). The probability model, (also called the surrogate or response surface), is easier to optimize than the actual objective function. Bayesian methods select the next values to evaluate by applying a criterion (usually Expected Improvement) to the surrogate. The concept is to limit the evaluations of the objective function by spending more time choosing the next values to try.  \n","\n","To conclude to the final model, we used five folds Cross-Validation for 100 epochs and early stopping with Bayesian Optimization. Also, we created a custom callback function so we can write proper logs that can be read by Tensorboard. This way we were able to monitor the validation process in real time.  \n","\n","The above was up to some point an iterative process. We run the optimization process until we noticed in Tensorboard that the models converge. Then we stopped and evaluated the results and move to the next iteration. (example of the process [here](https://www.kaggle.com/yannisp/sf-crime-analysis-prediction-optimiz-ex)) \n","\n","First we optimized a few basic hyperparameters including:\n","* `Boosting` selection between gbdt and dart\n","* `Max_delta_step` uniformly in the range  [0, 2]\n","* `Min_data_in_leaf` uniformly in the range [10,30]\n","* `Num_leaves` uniformly in the range [20,40]  \n","\n","After the model converged, a second round of tuning followed:\n","* `Boosting`: gbdt\n","* `Max_delta_step` uniformly in the range  [0.5, 2.5]\n","* `Min_data_in_leaf` uniformly in the range [10, 25]\n","* `Num_leaves` uniformly in the range [20, 45]\n","* `Max_bin` uniformly in the range [200, 500],\n","* `Learning_rate` uniformly in the range [0.1, 2]\n","\n","Finally we concluded to the following hyperparameters:  \n","* `Boosting`: gbdt\n","* `Max_delta_step`: 0.9\n","* `Min_data_in_leaf`:  21\n","* `Num_leaves`: 41\n","* `Max_bin`: 465,\n","* `Learning_rate`: 0.4  \n","\n","In the following figure, we present the performance of the best model from each step of optimization.  \n","![](https://i.imgur.com/grRIpi5.png?1)\n"]},{"cell_type":"markdown","metadata":{"_uuid":"4a94ee3d432d6bc02d8ab35f9007c3b682b9c5d9"},"source":["## Building the final model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1b675ca8c60ee991f7121c31d067a76713b315e5","trusted":true},"outputs":[],"source":["# Loading the data\n","train = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\n","test = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n","\n","# Data cleaning\n","train.drop_duplicates(inplace=True)\n","train.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n","test.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n","\n","imp = SimpleImputer(strategy='mean')\n","\n","for district in train['PdDistrict'].unique():\n","    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n","        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n","    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n","        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\n","train_data = lgb.Dataset(\n","    train, label=y, categorical_feature=['PdDistrict'], free_raw_data=False)\n","\n","# Feature Engineering\n","train = feature_engineering(train)\n","train.drop(columns=['Descript','Resolution'], inplace=True)\n","test = feature_engineering(test)\n","\n","# Encoding the Categorical Variables\n","le1 = LabelEncoder()\n","train['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n","test['PdDistrict'] = le1.transform(test['PdDistrict'])\n","\n","le2 = LabelEncoder()\n","X = train.drop(columns=['Category'])\n","y= le2.fit_transform(train['Category'])\n","\n","# Creating the model\n","train_data = lgb.Dataset(\n","    X, label=y, categorical_feature=['PdDistrict'])\n","\n","params = {'boosting':'gbdt',\n","          'objective':'multiclass',\n","          'num_class':39,\n","          'max_delta_step':0.9,\n","          'min_data_in_leaf': 21,\n","          'learning_rate': 0.4,\n","          'max_bin': 465,\n","          'num_leaves': 41\n","         }\n","\n","bst = lgb.train(params, train_data, 100)\n","\n","predictions = bst.predict(test)\n","\n","# Submitting the results\n","submission = pd.DataFrame(\n","    predictions,\n","    columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')),\n","    index=test.index)\n","submission.to_csv(\n","    'LGBM_final.csv', index_label='Id')"]},{"cell_type":"markdown","metadata":{"_uuid":"ed146225d56f334349b49ea9397d61cd4633316a"},"source":["# Model Evaluation and Validation\n","The final model scored 2.25697 on the training set which is 16% lower from the naive prediction (2.68015)  and 2% better than the benchmark. Taking into account the low margin between the naive and the benchmark we knew that we would probably have a small improvement.  \n","\n","That being said, we could say that the results are satisfactory.  \n","\n","Based on the Permutation Importance analysis we performed before, the model should be susceptible to changes in Minute and the coordinates and less sensitive to changes in Day, Year or Day of the week.  \n","\n","Indeed, by removing the ‘Minute’ feature from the dataset, we had an increase of loss to 2.53743 and by removing the ‘DayOfWeek’ feature the loss increased to 2.25900.  \n","\n","The Permutation importance is a great tool to understand **how much** a specific feature affect our prediction but it does not tell us anything about **the direction** it affects it. We can solve this issue and understand even deeper our model by using Partial Dependencies. In other words, if for an incident we change only the value of one feature how will this affect the probability of each crime category?  \n","\n","As an example, we can evaluate how the Hour affects the probabilities of three different crimes. We can see that the hour does not affect the probability for BRIBERY (class 3). In contrast, during the night the probability for BURGLARY (class 4) increases (up to 2%), and during the day the probability for DISORDERLY CONDUCT (class 5) decreases.  \n","\n","We can conclude that the model is aligned with our intuition.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"44af3dc239d812c8e0802deb40c55c162d532afe","trusted":true},"outputs":[],"source":["model = LGBMClassifier(**params).fit(X, y, categorical_feature=['PdDistrict'])\n","\n","pdp_Pd = pdp.pdp_isolate(\n","    model=model,\n","    dataset=X,\n","    model_features=X.columns.tolist(),\n","    feature='Hour',\n","    n_jobs=-1)\n","\n","pdp.pdp_plot(\n","    pdp_Pd,\n","    'Hour',\n","    ncols=3)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"9983e427f0c4e3102567329bf29befbbd3a72428"},"source":["# Conclusion\n","## Free-Form Visualization\n","An interesting visualization would be to depict how each feature affects a specific prediction. Insights like this are possible with the use of the SHAP library. \n","\n","SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the [SHAP NIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) for details).  \n","\n","As an example let’s select a row from the testing dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"22c2a8c4bca9bf236ed26ae1220bd5103bd4a748","trusted":true},"outputs":[],"source":["data_for_prediction = test.loc[[846262]]\n","data_for_prediction"]},{"cell_type":"markdown","metadata":{"_uuid":"893470e9059d287fc12fcae73fbb33a74627640d"},"source":["This incident has taken place in 03:30 in the night in a block. As we saw in the Partial Dependencies graphs before, BURGLARY has a higher probability and burglaries happen by definition in blocks. Let’s see if our model aligns with our intuition."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"be0c20f2881770ab3ba244fd9d556a84145922a1","trusted":true},"outputs":[],"source":["shap.initjs()\n","\n","# Create object that can calculate shap values\n","explainer = shap.TreeExplainer(model)\n","\n","# Calculate Shap values\n","shap_values = explainer.shap_values(data_for_prediction)\n","\n","shap.force_plot(explainer.expected_value[4], shap_values[4], data_for_prediction, link='logit')"]},{"cell_type":"markdown","metadata":{"_uuid":"8aa4eba243aeffd49f90ae7275e3abc53094b70e"},"source":["We can see that there is a 10% probability for BURGLARY and that this is mostly increased  because it takes place to a block (not on a crossroad) and from the time (hour and minute). Both of these are aligned again with our intuition, making us more confident about the validity of our model."]},{"cell_type":"markdown","metadata":{"_uuid":"b8d7d231cfb8eb22370a63719ccfa10fcc1dd26b"},"source":["## Reflection\n","As described in the previous sections a full cycle data processing have been followed and lead us to a satisfactory prediction model.  \n","\n","The most challenging part was that due to the nature of the features, there was a little room for feature engineering. For this reason, we had to be creative and use advanced techniques during the hyperparameter optimization to make a difference.  \n","This is a hard problem to solve with a heavily unbalanced dataset and the unpredictability (up to some point) of the “human factor”.  \n","## Improvements\n","We are sure there is space for improvement. Two additional techniques we would like to implement if there was the necessary time would be:  \n","\n","* Create **ordinal representations** for the features that present a kind of cyclicity (Month, Weekday, Hour, Minute). The reasoning behind this is that if we take the Hour as an example, the default representation implies that 23 and 00 (midnight) are 23 “units” away although in reality, they are 1 “unit” apart. A way to solve it is to imagine the hour in a real clock and take their projections on the axes passing from the center of the clock. This way the distance between 23 and 00 is the same as between 00 and 01. We can achieve this with functions like Hx = sin(2\\*π\\*H/23) & Hy = cos(2\\*π\\*H/23) for the hour and accordingly for the rest.    \n","\n","* Use **embeddings** or any other text processing technique, for the addresses. By extracting if the incident has happened to a block or a crossroad we have extracted the minimum gain from this feature, and maybe there are some patterns to exploit and give us even better score."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat":4,"nbformat_minor":1}
